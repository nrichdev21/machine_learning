{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://MSI:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1605307422953)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.ml.feature.{PCA, StandardScaler, VectorAssembler}\r\n",
       "import org.apache.spark.ml.linalg.Vectors\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6b8c244b\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature.{PCA,StandardScaler,VectorAssembler}\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"PCA_Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [mean radius: int, mean texture: double ... 28 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"../../data/ml_scala/Cancer_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mean radius: integer (nullable = true)\n",
      " |-- mean texture: double (nullable = true)\n",
      " |-- mean perimeter: double (nullable = true)\n",
      " |-- mean area: double (nullable = true)\n",
      " |-- mean smoothness: double (nullable = true)\n",
      " |-- mean compactness: double (nullable = true)\n",
      " |-- mean concavity: double (nullable = true)\n",
      " |-- mean concave points: double (nullable = true)\n",
      " |-- mean symmetry: double (nullable = true)\n",
      " |-- mean fractal dimension: double (nullable = true)\n",
      " |-- radius error: double (nullable = true)\n",
      " |-- texture error: double (nullable = true)\n",
      " |-- perimeter error: double (nullable = true)\n",
      " |-- area error: double (nullable = true)\n",
      " |-- smoothness error: double (nullable = true)\n",
      " |-- compactness error: double (nullable = true)\n",
      " |-- concavity error: double (nullable = true)\n",
      " |-- concave points error: double (nullable = true)\n",
      " |-- symmetry error: double (nullable = true)\n",
      " |-- fractal dimension error: double (nullable = true)\n",
      " |-- worst radius: double (nullable = true)\n",
      " |-- worst texture: double (nullable = true)\n",
      " |-- worst perimeter: double (nullable = true)\n",
      " |-- worst area: double (nullable = true)\n",
      " |-- worst smoothness: double (nullable = true)\n",
      " |-- worst compactness: double (nullable = true)\n",
      " |-- worst concavity: double (nullable = true)\n",
      " |-- worst concave points: double (nullable = true)\n",
      " |-- worst symmetry: double (nullable = true)\n",
      " |-- worst fractal dimension: double (nullable = true)\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colnames: Array[String] = Array(mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, worst fractal dimension)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colnames = (Array(\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\", \"mean smoothness\",\n",
    "\"mean compactness\", \"mean concavity\", \"mean concave points\", \"mean symmetry\", \"mean fractal dimension\",\n",
    "\"radius error\", \"texture error\", \"perimeter error\", \"area error\", \"smoothness error\", \"compactness error\",\n",
    "\"concavity error\", \"concave points error\", \"symmetry error\", \"fractal dimension error\", \"worst radius\",\n",
    "\"worst texture\", \"worst perimeter\", \"worst area\", \"worst smoothness\", \"worst compactness\", \"worst concavity\",\n",
    "\"worst concave points\", \"worst symmetry\", \"worst fractal dimension\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_87668d021cf6, handleInvalid=error, numInputCols=30\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(colnames).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [features: vector]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = assembler.transform(df).select($\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[0.0,17.99,10.38,...|[0.0,5.1049235941...|\n",
      "|[1.0,20.57,17.77,...|[0.00608270930682...|\n",
      "|[2.0,19.69,21.25,...|[0.01216541861364...|\n",
      "|[3.0,11.42,20.38,...|[0.01824812792047...|\n",
      "|[4.0,20.29,14.34,...|[0.02433083722729...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_5254a6b3ac1a\r\n",
       "scalerModel: org.apache.spark.ml.feature.StandardScalerModel = StandardScalerModel: uid=stdScal_5254a6b3ac1a, numFeatures=30, withMean=false, withStd=true\r\n",
       "scaledData: org.apache.spark.sql.DataFrame = [features: vector, scaledFeatures: vector]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// scaledFeatures\n",
    "val scaler = (new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false))\n",
    "\n",
    "// Standardize Features\n",
    "val scalerModel = scaler.fit(output)\n",
    "\n",
    "val scaledData = scalerModel.transform(output)\n",
    "\n",
    "scaledData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         pcaFeatures|\n",
      "+--------------------+\n",
      "|[21.6219973823649...|\n",
      "|[15.1217370347583...|\n",
      "|[18.4325856097778...|\n",
      "|[18.9549565028940...|\n",
      "|[16.7333072691963...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pca: org.apache.spark.ml.feature.PCAModel = PCAModel: uid=pca_ad8b7eeecd91, k=4\r\n",
       "pcaDF: org.apache.spark.sql.DataFrame = [features: vector, scaledFeatures: vector ... 1 more field]\r\n",
       "result: org.apache.spark.sql.DataFrame = [pcaFeatures: vector]\r\n",
       "res2: Array[org.apache.spark.sql.Row] = Array([[21.62199738236499,8.516595739466716,-3.731847417579699,-0.41812449701477833]])\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use PCA to use only four Principal components\n",
    "val pca = (new PCA()\n",
    "  .setInputCol(\"scaledFeatures\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(4)\n",
    "  .fit(scaledData))\n",
    "\n",
    "val pcaDF = pca.transform(scaledData)\n",
    "\n",
    "val result = pcaDF.select(\"pcaFeatures\")\n",
    "result.show(5)\n",
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
