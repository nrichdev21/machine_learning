{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://MSI:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1605214207529)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, RandomForestClassifier, GBTClassifier}\r\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}\r\n",
       "import org.apache.spark.ml.linalg.Vectors\r\n",
       "import org.apache.spark.ml.Pipeline\r\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\r\n",
       "import org.apache.spark.ml.feature.StandardScaler\r\n",
       "import org.apache.log4j._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7d5bdaa\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, RandomForestClassifier, GBTClassifier}\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Daily Time Spent on Site: double, Age: int ... 8 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = (spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\")\n",
    "          .option(\"multiline\",\"true\").format(\"csv\")\n",
    "          .load(\"../../data/ml_scala/advertising.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Daily Time Spent on Site: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Area Income: double (nullable = true)\n",
      " |-- Daily Internet Usage: double (nullable = true)\n",
      " |-- Ad Topic Line: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Male: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Clicked on Ad: integer (nullable = true)\n",
      "\n",
      "+-------+------------------------+-----------------+------------------+--------------------+--------------------+----------+-------------------+-----------+-------------------+------------------+\n",
      "|summary|Daily Time Spent on Site|              Age|       Area Income|Daily Internet Usage|       Ad Topic Line|      City|               Male|    Country|          Timestamp|     Clicked on Ad|\n",
      "+-------+------------------------+-----------------+------------------+--------------------+--------------------+----------+-------------------+-----------+-------------------+------------------+\n",
      "|  count|                    1000|             1000|              1000|                1000|                1000|      1000|               1000|       1000|               1000|              1000|\n",
      "|   mean|       65.00020000000012|           36.009| 55000.00008000003|  180.00010000000003|                null|      null|              0.481|       null|               null|               0.5|\n",
      "| stddev|      15.853614567500212|8.785562310125924|13414.634022282358|    43.9023393019801|                null|      null|0.49988887654046543|       null|               null|0.5002501876563867|\n",
      "|    min|                    32.6|               19|           13996.5|              104.78|Adaptive 24hour G...| Adamsbury|                  0|Afghanistan|2016-01-01 02:52:10|                 0|\n",
      "|    max|                   91.43|               61|           79484.8|              269.96|Visionary recipro...|Zacharyton|                  1|   Zimbabwe|2016-07-24 00:22:16|                 1|\n",
      "+-------+------------------------+-----------------+------------------+--------------------+--------------------+----------+-------------------+-----------+-------------------+------------------+\n",
      "\n",
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+\n",
      "|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|       Ad Topic Line|          City|Male|   Country|          Timestamp|Clicked on Ad|\n",
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+\n",
      "|                   68.95| 35|    61833.9|              256.09|Cloned 5thgenerat...|   Wrightburgh|   0|   Tunisia|2016-03-27 00:53:11|            0|\n",
      "|                   80.23| 31|   68441.85|              193.77|Monitored nationa...|     West Jodi|   1|     Nauru|2016-04-04 01:39:02|            0|\n",
      "|                   69.47| 26|   59785.94|               236.5|Organic bottom-li...|      Davidton|   0|San Marino|2016-03-13 20:35:42|            0|\n",
      "|                   74.15| 29|   54806.18|              245.89|Triple-buffered r...|West Terrifurt|   1|     Italy|2016-01-10 02:31:19|            0|\n",
      "|                   68.37| 35|   73889.99|              225.58|Robust logistical...|  South Manuel|   0|   Iceland|2016-06-03 03:36:18|            0|\n",
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "\n",
      "Example Data Row\n",
      "Age\n",
      "35\n",
      "\n",
      "\n",
      "Area Income\n",
      "61833.9\n",
      "\n",
      "\n",
      "Daily Internet Usage\n",
      "256.09\n",
      "\n",
      "\n",
      "Ad Topic Line\n",
      "Cloned 5thgeneration orchestration\n",
      "\n",
      "\n",
      "City\n",
      "Wrightburgh\n",
      "\n",
      "\n",
      "Male\n",
      "0\n",
      "\n",
      "\n",
      "Country\n",
      "Tunisia\n",
      "\n",
      "\n",
      "Timestamp\n",
      "2016-03-27 00:53:11\n",
      "\n",
      "\n",
      "Clicked on Ad\n",
      "0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colnames: Array[String] = Array(Daily Time Spent on Site, Age, Area Income, Daily Internet Usage, Ad Topic Line, City, Male, Country, Timestamp, Clicked on Ad)\r\n",
       "firstrow: org.apache.spark.sql.Row = [68.95,35,61833.9,256.09,Cloned 5thgeneration orchestration,Wrightburgh,0,Tunisia,2016-03-27 00:53:11,0]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.describe().show()\n",
    "df.show(5)\n",
    "\n",
    "// Print a row to better visualize\n",
    "val colnames = df.columns\n",
    "val firstrow = df.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+----+\n",
      "|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|       Ad Topic Line|          City|Male|   Country|          Timestamp|Clicked on Ad|Hour|\n",
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+----+\n",
      "|                   68.95| 35|    61833.9|              256.09|Cloned 5thgenerat...|   Wrightburgh|   0|   Tunisia|2016-03-27 00:53:11|            0|   0|\n",
      "|                   80.23| 31|   68441.85|              193.77|Monitored nationa...|     West Jodi|   1|     Nauru|2016-04-04 01:39:02|            0|   1|\n",
      "|                   69.47| 26|   59785.94|               236.5|Organic bottom-li...|      Davidton|   0|San Marino|2016-03-13 20:35:42|            0|  20|\n",
      "|                   74.15| 29|   54806.18|              245.89|Triple-buffered r...|West Terrifurt|   1|     Italy|2016-01-10 02:31:19|            0|   2|\n",
      "|                   68.37| 35|   73889.99|              225.58|Robust logistical...|  South Manuel|   0|   Iceland|2016-06-03 03:36:18|            0|   3|\n",
      "+------------------------+---+-----------+--------------------+--------------------+--------------+----+----------+-------------------+-------------+----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_time: org.apache.spark.sql.DataFrame = [Daily Time Spent on Site: double, Age: int ... 9 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Add an hour column from the Timestamp\n",
    "val df_time = df.withColumn(\"Hour\", hour(df(\"Timestamp\")))\n",
    "df_time.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.DataFrame = [Daily Time Spent on Site: double, Age: int ... 9 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop rows with null\n",
    "df_time.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(Daily Time Spent on Site, Age, Area Income, Daily Internet Usage, Ad Topic Line, City, Male, Country, Timestamp, Clicked on Ad, Hour)\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------+---+-----------+--------------------+----+----+\n",
      "|label|Daily Time Spent on Site|Age|Area Income|Daily Internet Usage|Male|Hour|\n",
      "+-----+------------------------+---+-----------+--------------------+----+----+\n",
      "|    0|                   68.95| 35|    61833.9|              256.09|   0|   0|\n",
      "|    0|                   80.23| 31|   68441.85|              193.77|   1|   1|\n",
      "|    0|                   69.47| 26|   59785.94|               236.5|   0|  20|\n",
      "|    0|                   74.15| 29|   54806.18|              245.89|   1|   2|\n",
      "|    0|                   68.37| 35|   73889.99|              225.58|   0|   3|\n",
      "+-----+------------------------+---+-----------+--------------------+----+----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_detail: org.apache.spark.sql.DataFrame = [label: int, Daily Time Spent on Site: double ... 5 more fields]\r\n",
       "df_clean: org.apache.spark.sql.DataFrame = [label: int, Daily Time Spent on Site: double ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select relevant features and label\n",
    "val df_detail = (df_time.select(df_time(\"Clicked on Ad\").as(\"label\"), $\"Daily Time Spent on Site\",\n",
    "                           $\"Age\",$\"Area Income\", $\"Daily Internet Usage\",$\"Male\",$\"Hour\"))\n",
    "\n",
    "// Remove rows with duplicates\n",
    "val df_clean = df_detail.na.drop()\n",
    "\n",
    "// Show dataframe\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_58eb290b3784, handleInvalid=error, numInputCols=6\r\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_64d9ab586d3d\r\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Daily Time Spent on Site: double ... 5 more fields]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Daily Time Spent on Site: double ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "                  .setInputCols(Array(\"Daily Time Spent on Site\", \"Age\", \"Area Income\",\n",
    "                                      \"Daily Internet Usage\",\"Male\",\"Hour\"))\n",
    "                  .setOutputCol(\"features\"))\n",
    "\n",
    "// Create scaler for random forest and gb classifiers\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "// Create train/test data sets\n",
    "val Array(training, test) = df_clean.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clf_lr: org.apache.spark.ml.classification.LogisticRegression = logreg_da1f93e812c1\r\n",
       "clf_rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_9c69ba19c931\r\n",
       "clf_gb: org.apache.spark.ml.classification.GBTClassifier = gbtc_634fb516776d\r\n",
       "log_pipeline: org.apache.spark.ml.Pipeline = pipeline_6bbeaf2aa945\r\n",
       "rf_pipeline: org.apache.spark.ml.Pipeline = pipeline_d316856f7c38\r\n",
       "gb_pipeline: org.apache.spark.ml.Pipeline = pipeline_1dd369a83bdb\r\n",
       "log_model: org.apache.spark.ml.PipelineModel = pipeline_6bbeaf2aa945\r\n",
       "rf_model: org.apache.spark.ml.PipelineModel = pipeline_d316856f7c38\r\n",
       "gb_model: org.apache.spark.ml.PipelineModel = pipeline_1dd369a83bdb\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create classifier\n",
    "val clf_lr = new LogisticRegression()\n",
    "val clf_rf = new RandomForestClassifier()\n",
    "val clf_gb = new GBTClassifier()\n",
    "\n",
    "// Create pipelines\n",
    "val log_pipeline = new Pipeline().setStages(Array(assembler,  clf_lr))\n",
    "val rf_pipeline = new Pipeline().setStages(Array(assembler, scaler, clf_rf))\n",
    "val gb_pipeline = new Pipeline().setStages(Array(assembler, scaler, clf_gb))\n",
    "\n",
    "// Fit models\n",
    "val log_model = log_pipeline.fit(training)\n",
    "val rf_model = rf_pipeline.fit(training)\n",
    "val gb_model =  gb_pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Model:\n",
      "156.0  1.0    \n",
      "9.0    144.0  \n",
      "Confusion Matrix for Random Forest Model:\n",
      "156.0  1.0    \n",
      "13.0   140.0  \n",
      "Confusion Matrix for Gradient Boosted Model:\n",
      "154.0  3.0    \n",
      "11.0   142.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "log_results: org.apache.spark.sql.DataFrame = [label: int, Daily Time Spent on Site: double ... 9 more fields]\r\n",
       "rf_results: org.apache.spark.sql.DataFrame = [label: int, Daily Time Spent on Site: double ... 10 more fields]\r\n",
       "gb_results: org.apache.spark.sql.DataFrame = [label: int, Daily Time Spent on Site: double ... 10 more fields]\r\n",
       "log_pred: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[616] at rdd at <console>:46\r\n",
       "rf_pred: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[622] at rdd at <console>:47\r\n",
       "gb_pred: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[628] at rdd at <console>:48\r\n",
       "log_metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@29c097cf\r\n",
       "rf_metrics: org.apache.spark.ml...\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Model Results\n",
    "val log_results = log_model.transform(test)\n",
    "val rf_results = rf_model.transform(test)\n",
    "val gb_results = gb_model.transform(test)\n",
    "\n",
    "// Convert to RDD to use the MulticlassMetrics class\n",
    "val log_pred = log_results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd\n",
    "val rf_pred = rf_results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd\n",
    "val gb_pred = gb_results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd\n",
    "\n",
    "// Create new metrics object\n",
    "val log_metrics = new MulticlassMetrics(log_pred)\n",
    "val rf_metrics = new MulticlassMetrics(rf_pred)\n",
    "val gb_metrics = new MulticlassMetrics(gb_pred)\n",
    "\n",
    "// Print out confusion matrices\n",
    "println(\"Confusion Matrix for Logistic Model:\")\n",
    "println(log_metrics.confusionMatrix)\n",
    "println(\"Confusion Matrix for Random Forest Model:\")\n",
    "println(rf_metrics.confusionMatrix)\n",
    "println(\"Confusion Matrix for Gradient Boosted Model:\")\n",
    "println(gb_metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
