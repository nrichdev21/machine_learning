{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://MSI:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1605303496590)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\r\n",
       "import org.apache.log4j._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6b8c244b\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: double, features: vector]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"libsvm\").load(\"../../data/ml_scala/sample_linear_regression_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split data for train/test\n",
    "val Array(train, test) = df.randomSplit(Array(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Exception thrown in awaitResult:\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Exception thrown in awaitResult:\r",
      "  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\r",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$6(TrainValidationSplit.scala:164)\r",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$6$adapted(TrainValidationSplit.scala:164)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:164)\r",
      "  at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r",
      "  at scala.util.Try$.apply(Try.scala:213)\r",
      "  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)\r",
      "  ... 39 elided\r",
      "Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\r",
      "This stopped SparkContext was created at:\r",
      "\r",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r",
      "py4j.Gateway.invoke(Gateway.java:238)\r",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\r",
      "java.base/java.lang.Thread.run(Thread.java:832)\r",
      "\r",
      "The currently active SparkContext was created at:\r",
      "\r",
      "(No active SparkContext.)\r",
      "\r",
      "  at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:111)\r",
      "  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1471)\r",
      "  at org.apache.spark.ml.source.libsvm.LibSVMFileFormat.buildReader(LibSVMRelation.scala:155)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\r",
      "  at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\r",
      "  at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:398)\r",
      "  at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:389)\r",
      "  at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:472)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r",
      "  at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r",
      "  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r",
      "  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r",
      "  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r",
      "  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\r",
      "  at org.apache.spark.sql.execution.SampleExec.inputRDDs(basicPhysicalOperators.scala:295)\r",
      "  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\r",
      "  at org.apache.spark.sql.execution.SampleExec.inputRDDs(basicPhysicalOperators.scala:295)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r",
      "  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:89)\r",
      "  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:65)\r",
      "  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:310)\r",
      "  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:135)\r",
      "  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:124)\r",
      "  at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:341)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r",
      "  at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r",
      "  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r",
      "  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r",
      "  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r",
      "  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r",
      "  at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\r",
      "  at org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3198)\r",
      "  at org.apache.spark.sql.Dataset.rdd(Dataset.scala:3196)\r",
      "  at org.apache.spark.ml.PredictorParams.extractInstances(Predictor.scala:80)\r",
      "  at org.apache.spark.ml.PredictorParams.extractInstances$(Predictor.scala:70)\r",
      "  at org.apache.spark.ml.Predictor.extractInstances(Predictor.scala:114)\r",
      "  at org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:323)\r",
      "  at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r",
      "  at scala.util.Try$.apply(Try.scala:213)\r",
      "  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r",
      "  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:319)\r",
      "  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:176)\r",
      "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\r",
      "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r",
      "  at org.apache.spark.ml.Estimator.fit(Estimator.scala:59)\r",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$4(TrainValidationSplit.scala:151)\r",
      "  at scala.runtime.java8.JFunction0$mcD$sp.apply(JFunction0$mcD$sp.java:23)\r",
      "  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\r",
      "  at scala.util.Success.$anonfun$map$1(Try.scala:255)\r",
      "  at scala.util.Success.map(Try.scala:213)\r",
      "  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\r",
      "  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r",
      "  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r",
      "  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r",
      "  at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:100)\r",
      "  at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\r",
      "  at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\r",
      "  at scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete(Promise.scala:372)\r",
      "  at scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete$(Promise.scala:371)\r",
      "  at scala.concurrent.impl.Promise$KeptPromise$Successful.onComplete(Promise.scala:379)\r",
      "  at scala.concurrent.impl.Promise.transform(Promise.scala:33)\r",
      "  at scala.concurrent.impl.Promise.transform$(Promise.scala:31)\r",
      "  at scala.concurrent.impl.Promise$KeptPromise$Successful.transform(Promise.scala:379)\r",
      "  at scala.concurrent.Future.map(Future.scala:292)\r",
      "  at scala.concurrent.Future.map$(Future.scala:292)\r",
      "  at scala.concurrent.impl.Promise$KeptPromise$Successful.map(Promise.scala:379)\r",
      "  at scala.concurrent.Future$.apply(Future.scala:659)\r",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$3(TrainValidationSplit.scala:160)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r",
      "  at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:149)\r",
      "  ... 43 more\r",
      ""
     ]
    }
   ],
   "source": [
    "// Create basic regression model\n",
    "val lr = new LinearRegression()\n",
    "\n",
    "// Create hyper-paramter grid for tuning\n",
    "val paramGrid = (new ParamGridBuilder().addGrid(lr.regParam, Array(0.1,0.01))\n",
    "                 .addGrid(lr.fitIntercept).addGrid(lr.elasticNetParam, Array(0.0,0.5,0.01))\n",
    "                 .build())\n",
    "\n",
    "// Validation for hyperparameter tuning - similiar to CV but only trains once\n",
    "val trainValidationSplit = (new TrainValidationSplit().setEstimator(lr).setEvaluator(new RegressionEvaluator)\n",
    "                            .setEstimatorParamMaps(paramGrid).setTrainRatio(0.8))\n",
    "\n",
    "//  Train the model\n",
    "val model = trainValidationSplit.fit(train)\n",
    "\n",
    "// Show predictions against training set\n",
    "model.transform(test).select(\"features\", \"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
