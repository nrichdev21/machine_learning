{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/nick/spark-3.0.1-bin-hadoop2.7')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('Spam Detection').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('Natural_Language_Processing/smsspamcollection/SMSSpamCollection',\n",
    "                     inferSchema=True, sep='\\t')\n",
    "data.show(5)\n",
    "data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "+-----+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('length',length(data['text']))\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
    "stop_remove = StopWordsRemover(inputCol='token_text', outputCol='stop_token')\n",
    "count_vec = CountVectorizer(inputCol='stop_token', outputCol='c_vec')\n",
    "idf = IDF(inputCol='c_vec', outputCol='tf_idf')\n",
    "ham_spam_to_numeric = StringIndexer(inputCol='class', outputCol='label')\n",
    "\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently for NLP use a Naive Bayes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dataprep = Pipeline(stages=[ham_spam_to_numeric,\n",
    "                                tokenizer,\n",
    "                                stop_remove,\n",
    "                                count_vec,\n",
    "                                idf,\n",
    "                                clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = pipe_dataprep.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'text',\n",
       " 'length',\n",
       " 'label',\n",
       " 'token_text',\n",
       " 'stop_token',\n",
       " 'c_vec',\n",
       " 'tf_idf',\n",
       " 'features']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = cleaner.transform(data)\n",
    "clean_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "|  0.0|(13424,[0,70,80,1...|\n",
      "|  0.0|(13424,[36,134,31...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data = clean_data.select(['label','features'])\n",
    "clean_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detector = nb.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,13,...|[-601.98424404768...|[1.0,6.6072904158...|       0.0|\n",
      "|  0.0|(13424,[0,1,2,41,...|[-1061.5876846148...|[1.0,9.2516301993...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,15,...|[-654.87738773247...|[1.0,5.5276227570...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[-536.18355575630...|[1.0,6.6123033884...|       0.0|\n",
      "|  0.0|(13424,[0,1,11,32...|[-872.69658898342...|[1.0,1.0702129072...|       0.0|\n",
      "|  0.0|(13424,[0,1,12,33...|[-444.93497434011...|[1.0,4.8381063288...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,31...|[-216.97987788817...|[1.0,2.2729672683...|       0.0|\n",
      "|  0.0|(13424,[0,1,31,43...|[-337.93079674438...|[1.0,5.2300980274...|       0.0|\n",
      "|  0.0|(13424,[0,1,43,69...|[-615.11841881311...|[0.99996222296317...|       0.0|\n",
      "|  0.0|(13424,[0,1,416,6...|[-297.94435311883...|[1.0,2.1450735627...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3288.0585958302...|[1.0,8.6462508967...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-2490.5785275324...|[1.0,5.3587334589...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-1616.9416662658...|[1.0,2.8421062886...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,1...|[-1311.5611437877...|[1.0,4.4209958549...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,10,...|[-1249.1254175064...|[1.0,4.9303822881...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,11,...|[-1128.1175835098...|[1.0,6.6408625533...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,40,...|[-1590.2995362446...|[0.99989151069146...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,8,3...|[-1145.5461815893...|[1.0,1.2631535073...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,43,...|[-597.36718038207...|[1.0,3.4053493681...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,60,...|[-1327.4275073317...|[1.0,6.4587556294...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = spam_detector.transform(test_data)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9176498128548239\n"
     ]
    }
   ],
   "source": [
    "accuracy_eval = MulticlassClassificationEvaluator()\n",
    "accuracy = accuracy_eval.evaluate(test_results)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
