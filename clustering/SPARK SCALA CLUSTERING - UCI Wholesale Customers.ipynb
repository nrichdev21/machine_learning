{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://MSI:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1605306413700)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.ml.clustering.{KMeans, KMeansSummary}\r\n",
       "import org.apache.spark.ml.evaluation.ClusteringEvaluator\r\n",
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.linalg.Vectors\r\n",
       "import org.apache.log4j._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@20b26d7a\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import org.apache.spark.ml.clustering.{KMeans, KMeansSummary}\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Channel: int, Region: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = (spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\")\n",
    "          .option(\"header\",\"true\").format(\"csv\").load(\"../../data/ml_scala/Wholesale customers data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Channel: integer (nullable = true)\n",
      " |-- Region: integer (nullable = true)\n",
      " |-- Fresh: integer (nullable = true)\n",
      " |-- Milk: integer (nullable = true)\n",
      " |-- Grocery: integer (nullable = true)\n",
      " |-- Frozen: integer (nullable = true)\n",
      " |-- Detergents_Paper: integer (nullable = true)\n",
      " |-- Delicassen: integer (nullable = true)\n",
      "\n",
      "+-------+------+-----+----+-------+------+----------------+----------+\n",
      "|Channel|Region|Fresh|Milk|Grocery|Frozen|Detergents_Paper|Delicassen|\n",
      "+-------+------+-----+----+-------+------+----------------+----------+\n",
      "|      2|     3|12669|9656|   7561|   214|            2674|      1338|\n",
      "|      2|     3| 7057|9810|   9568|  1762|            3293|      1776|\n",
      "|      2|     3| 6353|8808|   7684|  2405|            3516|      7844|\n",
      "|      1|     3|13265|1196|   4221|  6404|             507|      1788|\n",
      "|      2|     3|22615|5410|   7198|  3915|            1777|      5185|\n",
      "+-------+------+-----+----+-------+------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|           Channel|            Region|             Fresh|              Milk|          Grocery|           Frozen|  Detergents_Paper|        Delicassen|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|               440|               440|               440|               440|              440|              440|               440|               440|\n",
      "|   mean|1.3227272727272728| 2.543181818181818|12000.297727272728| 5796.265909090909|7951.277272727273|3071.931818181818|2881.4931818181817|1524.8704545454545|\n",
      "| stddev|0.4680515694791137|0.7742724492301002|12647.328865076885|7380.3771745708445|9503.162828994346|4854.673332592367| 4767.854447904201|2820.1059373693965|\n",
      "|    min|                 1|                 1|                 3|                55|                3|               25|                 3|                 3|\n",
      "|    max|                 2|                 3|            112151|             73498|            92780|            60869|             40827|             47943|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+------+----------------+----------+\n",
      "|Fresh|Milk|Grocery|Frozen|Detergents_Paper|Delicassen|\n",
      "+-----+----+-------+------+----------------+----------+\n",
      "|12669|9656|   7561|   214|            2674|      1338|\n",
      "| 7057|9810|   9568|  1762|            3293|      1776|\n",
      "| 6353|8808|   7684|  2405|            3516|      7844|\n",
      "|13265|1196|   4221|  6404|             507|      1788|\n",
      "|22615|5410|   7198|  3915|            1777|      5185|\n",
      "+-----+----+-------+------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_features: org.apache.spark.sql.DataFrame = [Fresh: int, Milk: int ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select features to use in the model\n",
    "val df_features = (df.select($\"Fresh\", $\"Milk\", $\"Grocery\",\n",
    "                            $\"Frozen\", $\"Detergents_Paper\",$\"Delicassen\"))\n",
    "\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[12669.0,9656.0,7...|\n",
      "|[7057.0,9810.0,95...|\n",
      "|[6353.0,8808.0,76...|\n",
      "|[13265.0,1196.0,4...|\n",
      "|[22615.0,5410.0,7...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_88cf2229d57e, handleInvalid=error, numInputCols=6\r\n",
       "training_data: org.apache.spark.sql.DataFrame = [features: vector]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create matrix of features\n",
    "val assembler = (new VectorAssembler()\n",
    "                 .setInputCols(Array(\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \"Detergents_Paper\", \"Delicassen\"))\n",
    "                 .setOutputCol(\"features\"))\n",
    "\n",
    "val training_data = assembler.transform(df).select(\"features\")\n",
    "\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_fa299a69dc5c\r\n",
       "model: org.apache.spark.ml.clustering.KMeansModel = KMeansModel: uid=kmeans_fa299a69dc5c, k=3, distanceMeasure=euclidean, numFeatures=6\r\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, prediction: int]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train a K-Means Model\n",
    "val kmeans = new KMeans().setK(3).setSeed(1L)\n",
    "val model = kmeans.fit(training_data)\n",
    "\n",
    "val predictions = model.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6482181662567144\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = ClusteringEvaluator: uid=cluEval_0a6d9e1fa408, metricName=silhouette, distanceMeasure=squaredEuclidean\r\n",
       "silhouette: Double = 0.6482181662567144\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new ClusteringEvaluator()\n",
    "\n",
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[7390.958456973294,4439.768545994066,6292.19584569733,2495.53412462908,2238.6528189910982,1158.4480712166173]\n",
      "[32768.013333333336,4827.68,5723.146666666667,5535.92,1074.1200000000001,2066.6400000000003]\n",
      "[11849.17857142857,24717.10714285714,33887.71428571428,3409.3214285714284,15459.714285714284,4483.857142857142]\n"
     ]
    }
   ],
   "source": [
    "// Show Cluster Centroids\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
